{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shounakacharya/Documents/Finetuning_llama_tut/llmenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#\"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "device = \"mps\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello what are you? I'm a human being. I don't know what you're. You're not from around here, are you? You\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "generation_pipeline(\"Hello what are you?\", max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"Hello what are you? I am a bot, and I'm here to help you with any questions or topics you'd like to discuss.\\n\\nAlso,\"}],\n",
       " [{'generated_text': \"How are you doing today? I hope you're having a great day so far.\\n\\nI'm doing well, thanks for asking! I was just thinking about\"}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can generate responses in batches\n",
    "generation_pipeline([\"Hello what are you?\", \"How are you doing today?\"], max_new_tokens=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   9906,   1148,    527,    499,   1437,  12825,     30],\n",
      "        [128009, 128009, 128000,   4289,   1148,    279,  20868,     30]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# seeing the tokenized input\n",
    "input_prompt = [\n",
    "        \"Hello what are you bruh?\",\n",
    "        \"erm what the sigma?\"\n",
    "    ]\n",
    "tokenized = tokenizer(input_prompt, padding = True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello what are you bruh?', 'erm what the sigma?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding the tokenized input\n",
    "tokenizer.batch_decode(tokenized[\"input_ids\"], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1419,   2947,    220,   2366,     20,    271,   2675,    527,\n",
      "            264,   7941,     77,  15592,  18328,    889,  21881,   1093,    264,\n",
      "          55066,     13, 128009, 128006,    882, 128007,    271,   3923,    374,\n",
      "            279,   6864,    315,  23683,    526,   3444,     30, 128009, 128006,\n",
      "          78191, 128007,    271]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Not all LLMs have an inbuilt chat template, the currrent \"meta-llama/Llama-3.2-1B-Instruct\" did but not \"meta-llama/Llama-3.2-1B\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smartn AI assistant who speaks like a pirate.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Telangana?\"\n",
    "    }\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = True,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device) \n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 Mar 2025\n",
      "\n",
      "You are a smartn AI assistant who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Telangana?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yer lookin' fer the capital o' Telangana, eh? Alright then, matey! The capital o' Telangana be Hyderabad! Yer can find it on the banks o' the River Hussain Sagar, in the heart o' the state. It be a bustlin' metropolis with a rich history and culture, and aye,\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])\n",
    "# eot = end of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1419,   2947,    220,   2366,     20,    271,   2675,    527,\n",
      "            264,   7941,     77,  15592,  18328,    889,  21881,   1093,    264,\n",
      "          55066,     13, 128009, 128006,    882, 128007,    271,   3923,    374,\n",
      "            279,   6864,    315,  23683,    526,   3444,     30, 128009, 128006,\n",
      "            395,  52044, 128007,    271,  61055,    264,   9188,   2107,  44886]],\n",
      "       device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n if you keep the tokenizer as:\\ntokenizer.apply_chat_template(\\n    prompt_template,\\n    add_generation_prompt = True,\\n    tokenize = False,\\n    padding = True,\\n    return_tensors = \"pt\"\\n)\\nand print the output, you get:\\n\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 23 Mar 2025\\n\\nYou are a smartn AI assistant who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of Telangana?<|eot_id|><|start_header_id|>assitant<|end_header_id|>\\n\\naye aye cap\\'n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nthis is the prompt thats going to be fed into the llm and the reference for it to generate. But notice how there are emtpy start-end header token\\nafter \"aye aye cap\\'n\", this means that the llm will generate a new message henceforth while our intention was for it to\\nSTART every message with that. So we need to make the following tweaks:\\n\\nwe set add generationt prompt to false (so that it doesnt add the new header after the last message) and set continue_final_message to true to \\nmake sure the llm continues generating from the last message onwards\\n\\nsee the updated tokenizer used above\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smartn AI assistant who speaks like a pirate.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Telangana?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assitant\",\n",
    "        \"content\": \"aye aye cap'n\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_fin = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt = False,\n",
    "    continue_final_message= True,\n",
    "    tokenize = True,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(device) \n",
    "\n",
    "print(tokenized_fin)\n",
    "\n",
    "''' \n",
    " if you keep the tokenizer as:\n",
    "tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = False,\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "and print the output, you get:\n",
    "\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 Mar 2025\n",
    "\n",
    "You are a smartn AI assistant who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is the capital of Telangana?<|eot_id|><|start_header_id|>assitant<|end_header_id|>\n",
    "\n",
    "aye aye cap'n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "this is the prompt thats going to be fed into the llm and the reference for it to generate. But notice how there are emtpy start-end header token\n",
    "after \"aye aye cap'n\", this means that the llm will generate a new message henceforth while our intention was for it to\n",
    "START every message with that. So we need to make the following tweaks:\n",
    "\n",
    "we set add generationt prompt to false (so that it doesnt add the new header after the last message) and set continue_final_message to true to \n",
    "make sure the llm continues generating from the last message onwards\n",
    "\n",
    "see the updated tokenizer used above\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 Mar 2025\n",
      "\n",
      "You are a smartn AI assistant who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Telangana?<|eot_id|><|start_header_id|>assitant<|end_header_id|>\n",
      "\n",
      "aye aye cap'n! Yer lookin' fer the capital o' Telangana, eh? Alright then, matey... the capital o' Telangana be Hyderabad, the great city o' gold and spice! Yer can find all sorts o' treasures 'round the city, like the famous Charminar and the Telangana State Museum. And don't ferget to try\n"
     ]
    }
   ],
   "source": [
    "out_fin = model.generate(tokenized_fin, max_new_tokens = 75)\n",
    "decoded_fin = tokenizer.batch_decode(out_fin)\n",
    "print(decoded_fin[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ghost on the Shell: An Expressive Representati...</td>\n",
       "      <td>The creation of photorealistic virtual worlds ...</td>\n",
       "      <td>[Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu,...</td>\n",
       "      <td>2023-10-23 17:59:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Handling Data Heterogeneity via Architectural ...</td>\n",
       "      <td>Federated Learning (FL) is a promising researc...</td>\n",
       "      <td>[Sara Pieri, Jose Renato Restom, Samuel Horvat...</td>\n",
       "      <td>2023-10-23 17:59:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear Representations of Sentiment in Large L...</td>\n",
       "      <td>Sentiment is a pervasive feature in natural la...</td>\n",
       "      <td>[Curt Tigges, Oskar John Hollinsworth, Atticus...</td>\n",
       "      <td>2023-10-23 17:55:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Verb Conjugation in Transformers Is Determined...</td>\n",
       "      <td>Deep architectures such as Transformers are so...</td>\n",
       "      <td>[Sophie Hao, Tal Linzen]</td>\n",
       "      <td>2023-10-23 17:53:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online Detection of AI-Generated Images</td>\n",
       "      <td>With advancements in AI-generated images comin...</td>\n",
       "      <td>[David C. Epstein, Ishan Jain, Oliver Wang, Ri...</td>\n",
       "      <td>2023-10-23 17:53:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ghost on the Shell: An Expressive Representati...   \n",
       "1  Handling Data Heterogeneity via Architectural ...   \n",
       "2  Linear Representations of Sentiment in Large L...   \n",
       "3  Verb Conjugation in Transformers Is Determined...   \n",
       "4            Online Detection of AI-Generated Images   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The creation of photorealistic virtual worlds ...   \n",
       "1  Federated Learning (FL) is a promising researc...   \n",
       "2  Sentiment is a pervasive feature in natural la...   \n",
       "3  Deep architectures such as Transformers are so...   \n",
       "4  With advancements in AI-generated images comin...   \n",
       "\n",
       "                                             authors            published  \n",
       "0  [Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu,...  2023-10-23 17:59:52  \n",
       "1  [Sara Pieri, Jose Renato Restom, Samuel Horvat...  2023-10-23 17:59:16  \n",
       "2  [Curt Tigges, Oskar John Hollinsworth, Atticus...  2023-10-23 17:55:31  \n",
       "3                           [Sophie Hao, Tal Linzen]  2023-10-23 17:53:47  \n",
       "4  [David C. Epstein, Ishan Jain, Oliver Wang, Ri...  2023-10-23 17:53:14  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/ashish-chouhan/arxiv_cs_papers/data/train-00000-of-00001-bf80d7e563046673.parquet\")\n",
    "df = df.iloc[:,:4]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the model predict the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     40,   2846,   2133,   1203,    311]], device='mps:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'm going back to\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "out = model(input_ids)\n",
    "input_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntoken representation of the input text (in order):\\nstart of sequence, hello, how, are, you\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "token representation of the input text (in order):\n",
    "start of sequence, hello, how, are, you\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape # last element is the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' my'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.logits.argmax(axis=-1)[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While finetuning a pretrained model, we should ideally apply loss over the answer and not the entire sequence (including the prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "    full_response_text = [\n",
    "        (chat_template + \" \" + target_response + tokenizer.eos_token)\n",
    "        for chat_template, target_response in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "\n",
    "    input_ids_tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    labels_tokenized = tokenizer([\" \" + response + tokenizer.eos_token for response in target_responses],\n",
    "                                 add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length= input_ids_tokenized.shape[1])[\"input_ids\"]\n",
    "    \n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenzied_right_shifted = labels_tokenized_fixed[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted.to(device),\n",
    "        \"labels\": labels_tokenzied_right_shifted.to(device),\n",
    "    }  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_input_output_pair(\n",
    "    prompt = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital of Telangana?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Capital:\"}\n",
    "        ]\n",
    "    ],\n",
    "    target_responses =[ \"Mumbai\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1419,   2947,    220,   2366,     20,    271, 128009, 128006,\n",
       "            882, 128007,    271,   3923,    374,    279,   6864,    315,  23683,\n",
       "            526,   3444,     30, 128009, 128006,  78191, 128007,    271,  64693,\n",
       "             25,  35812]], device='mps:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 35812,  -100]], device='mps:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids = data['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        9.5625, -0.0000], device='mps:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(out.logits, data['labels'].to(device)) # notice how changing the target response to \"Mumbai\" from \"Hyderabad\" changes the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 Mar 2025\n",
      "\n",
      "user\n",
      "\n",
      "I'm going back toassistant\n",
      "\n",
      "This lyric ends with: \"back to the beginning\"\n"
     ]
    }
   ],
   "source": [
    "training_prompt = [\n",
    "    {\n",
    "        \"role\":\"user\", \"content\": \"I'm going back to\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\", \"content\": \"This lyric ends with:\"\n",
    "    }\n",
    "]\n",
    "training_target_response = \"505\"\n",
    "\n",
    "test_tokenized = tokenizer.apply_chat_template(training_prompt, continue_final_message=True, tokenize=True, return_tensors=\"pt\").to(device)\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=75)\n",
    "print(tokenizer.batch_decode(test_out,skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.423828\n",
      "Epoch 2, Loss: 0.128906\n",
      "Epoch 3, Loss: 0.380859\n",
      "Epoch 4, Loss: 0.198242\n",
      "Epoch 5, Loss: 0.045166\n",
      "Epoch 6, Loss: 0.451172\n",
      "Epoch 7, Loss: 0.007568\n",
      "Epoch 8, Loss: 0.005951\n",
      "Epoch 9, Loss: 0.005981\n",
      "Epoch 10, Loss: 0.000431\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Generate training data once outside the loop\n",
    "training_data = generate_input_output_pair(prompt=[training_prompt], target_responses=[training_target_response])\n",
    "training_data[\"input_ids\"] = training_data[\"input_ids\"].to(device)\n",
    "training_data[\"labels\"] = training_data[\"labels\"].to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    out = model(input_ids=training_data[\"input_ids\"])\n",
    "    loss = calculate_loss(out.logits, training_data[\"labels\"]).mean()\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 Mar 2025\n",
      "\n",
      "user\n",
      "\n",
      "I'm going back toassistant\n",
      "\n",
      "This lyric ends with: 505 505 505 505  \n"
     ]
    }
   ],
   "source": [
    "pred_tokenized = tokenizer.apply_chat_template(training_prompt, continue_final_message=True, tokenize=True, return_tensors=\"pt\").to(device)\n",
    "pred_out = model.generate(pred_tokenized, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(pred_out,skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "lora_tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side = \"left\")\n",
    "lora_tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_lora = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16).to(device)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.423828\n",
      "Epoch 2, Loss: 0.302734\n",
      "Epoch 3, Loss: 0.214844\n",
      "Epoch 4, Loss: 0.081055\n",
      "Epoch 5, Loss: 0.010742\n",
      "Epoch 6, Loss: 0.005707\n",
      "Epoch 7, Loss: 0.000645\n",
      "Epoch 8, Loss: 0.001556\n",
      "Epoch 9, Loss: 0.000299\n",
      "Epoch 10, Loss: 0.000105\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "lora_optimizer = AdamW(model_lora.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # Forward pass\n",
    "    out = model_lora(input_ids=training_data[\"input_ids\"])\n",
    "    loss = calculate_loss(out.logits, training_data[\"labels\"]).mean()\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    lora_optimizer.step()\n",
    "    lora_optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
